# VLLM Server Configuration for Independent Rollouts
# This configuration enables the new independent rollout architecture

defaults:
  - base

# System configuration
system:
  CUDA_VISIBLE_DEVICES: "0,1,2,3"

# Model configuration
model_path: Qwen/Qwen2.5-7B-Instruct

# Enable independent rollout mode
ray_rollout:
  use_ray: true
  rollout_mode: independent  # Use 'independent' for VLLM server + independent workers
  
  # Environment worker configuration
  num_env_workers: 8  # Number of independent environment workers
  
  # Ray cluster configuration
  ray_init_config:
    num_cpus: 16
    num_gpus: 0  # Don't reserve GPUs for Ray - let VLLM manage them directly
    object_store_memory: 4000000000  # 4GB object store
    
  # Environment worker settings
  env_worker:
    max_envs_per_worker: 4  # Environments per worker
    num_cpus_per_worker: 2  # CPU cores per worker
    num_gpus_per_worker: 0  # Workers don't need GPUs (API calls)
    
  # Performance settings
  performance:
    observation_batch_size: 32
    ray_timeout: 300
    
# VLLM API Server Configuration
vllm_server:
  # Server network settings
  host: "0.0.0.0"
  port: 8002  # Changed port to avoid conflicts
  route_prefix: "/v1"
  
  # Hardware configuration
  accelerator_type: "H100"  # GPU type: A100, H100, V100, etc.
  
  # Auto-scaling settings
  min_replicas: 1
  max_replicas: 2
  target_requests_per_replica: 8
  
  # Server health monitoring
  health_check_interval: 30
  restart_on_failure: true
  max_restart_attempts: 3

# Override rollout settings for independent mode
actor_rollout_ref:
  model:
    path: ${model_path}
  rollout:
    # VLLM server settings (these override base.yaml)
    tensor_model_parallel_size: 2
    gpu_memory_utilization: 0.8
    max_model_len: 4000
    response_length: 500
    max_num_seqs: 32
    enable_chunked_prefill: true
    enable_prefix_caching: true
    enforce_eager: true
    max_num_batched_tokens: 8192
    
    # Generation parameters for API calls
    temperature: 0.7
    top_p: 0.9
    top_k: 50
    do_sample: true

# Environment-specific overrides for independent rollout
es_manager:
  train:
    # Reduce group size for faster testing
    group_size: 8
  val:
    # Validation settings
    group_size: 4

# Agent settings optimized for API calls
agent_proxy:
  max_turn: 8  # Allow more turns for complex environments
  action_sep: "||"
  max_actions_per_turn: 3
  
# API-based LLM configuration (fallback if needed)
model_config:
  model_name: ragen_model  # Model name for VLLM server
  max_concurrency: 16  # High concurrency for independent workers

model_info:
  ragen_model:
    provider_name: vllm_local  # Use local VLLM server
    model_name: ragen_model
    base_url: "http://localhost:8002/v1"  # Updated port
    generation_kwargs:
      temperature: 0.7
      max_tokens: 500
      top_p: 0.9
      top_k: 50